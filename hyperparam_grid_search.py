# -*- coding: utf-8 -*-
"""GPT Hyperparam Tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N_kqFW4HIe_iAz8Mm_fZMEvEpzE3ongt

# Hyperparameter Grid Search

This is a grid search over candidate values for the four parameters below, with 3 values/parameter.

The code makes 2 API calls per param combination for each lesson. One to get the answer to a user's question, one asking the model to rate the answer, given the question and the answer gotten.

### **temperature**
What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
--> we want more deterministic: [0.3, 1]

### **top_p**
An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
--> same, more deterministic [0.3, 0.5]

### **presence_penalty**
Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
--> since chatGPt will need to reuse coding jargon, we should not penalize word repetition: [-0.5, 0.5]

### **frequency_penalty**
Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
--> the lesson text is likely to repeat the keyword (like "loops") and we don't want to overpenalize, so: [-1, 0]
"""

!pip install --upgrade openai

import openai
import pandas as pd
import time

# Set up OpenAI API credentials
openai.api_key = ''

# Define the parameter values to search over
TEMPERATURES = [0.3, 1]
TOP_PS = [0.3, 0.5]
PRESENCE_PENALTIES = [-0.5, 0.5]
FREQUENCY_PENALTIES = [-1, 0]

# Define the prompt and other API parameters
default_question1 = 'Explain what I should do at this lesson'
default_question2 = 'Give me another example for this concept'
default_question3 = 'Explain this in another way'


ENGINE = 'gpt-4'
MAX_TOKENS = 100

def evaluate_response(question, response):
    '''
    Here we send the model the user's asked question, its answer, and ask it to self-evaluate.
    '''
    time.sleep(30)
    # Get the generated answer from the response
    answer = response.choices[0].message.content

    # 
    # Generate the final prompt with the question and generated answer
    eval_prompt = f'Given the question and answer provided, please rate the answer in terms of: how helpful it is for a beginner, how on-point it is in relation to the question.\
                    Give me an answer with the format number#reason, where number is a score from 1 to 5 (where 5 is the best score). Give me the number, followed by a "#" sign, and then an explanation of your rating choice in one short sentence. \
                    Question: {question}\n \
                    Answer: {answer}'

    # Call the OpenAI API to evaluate the response
    evaluation = response = openai.ChatCompletion.create(
                      model='gpt-3.5-turbo',
                      messages=[
                        {"role": "user", "content": eval_prompt}
                      ],
                      temperature=0,
                      max_tokens = 50
                  )

    # Get the evaluation score from the response
    eval_output = evaluation.choices[0].message.content
    print(eval_output)
    score = float(eval_output.split('#')[0])
    reason = eval_output.split('#')[1].strip()

    return {'score': score, 'reason': reason}

def make_user_prompt(concept, language, lesson_text, lesson_code, user_question):
  lesson_content = f'{lesson_text} \n Code: \n "{lesson_code}"'
  prompt = f'This user is a beginner learning {concept} in {language}. Act as a tutor. Give hints, but not the final solution.\n \
    Any code example should be maximum 10 lines long, and runnable. Your answer text should be around 1-3 sentences long.\n \
    Avoid introducing topics that the user might be unfamiliar with.\n \
    Content of the current lesson: \"{lesson_content}\"\n\
    Answer the user’s question about this lesson: \“{user_question}\”.\n\
    If the user’s question is unrelated to the lesson, tell them that you answer only lesson-related questions, asking them to try again.'
  
  return prompt

def param_search_for_example(results_df, concept, language, lesson_text, lesson_code, user_question):
  
  # make the prompt that will be sent to chatGPT as the user's question
  prompt = make_user_prompt(concept, language, lesson_text, lesson_code, user_question)

  # Perform grid search
  best_score = float('-inf')
  best_params = {}

  for temp in TEMPERATURES:
      for top_p in TOP_PS:
          for presence_penalty in PRESENCE_PENALTIES:
              for frequency_penalty in FREQUENCY_PENALTIES:
                  # Call the OpenAI API
                  response = openai.ChatCompletion.create(
                      model=ENGINE,
                      messages=[
                        {"role": "user", "content": prompt}
                      ],
                      temperature=temp,
                      top_p=top_p,
                      presence_penalty=presence_penalty,
                      frequency_penalty=frequency_penalty,
                      max_tokens=MAX_TOKENS,
                  )

                  time.sleep(30)
                  
                  # Evaluate the response and update best parameters if necessary
                  eval_resp = evaluate_response(prompt, response)

                  if eval_resp['score'] > best_score:
                      best_score = eval_resp['score']
                      best_params['temperature'] = temp
                      best_params['top_p'] = top_p
                      best_params['presence_penalty'] = presence_penalty
                      best_params['frequency_penalty'] = frequency_penalty

                  new_row = pd.DataFrame({'#temperature': [temp], '#top_p': [top_p], 
                                                  '#presence_penalty': [presence_penalty], '#freq_penalty': [frequency_penalty],
                                                  'Concept': [concept], 'Question': [prompt], 'GPT_Answer': [response.choices[0].message.content], 
                                                  'Rating': [eval_resp['score']], 'Rating_Explanation': [eval_resp['reason']]})
                  score = eval_resp['score']
                  print(f'{temp}, {top_p}, {presence_penalty}, {frequency_penalty} --> score: {score}')
                  results_df = pd.concat([results_df, new_row], ignore_index=True)
                  results_df.to_csv('temp_lesson_log.csv')
                                          
  return {'best_params': best_params, 'best_score': best_score, 'results_df': results_df}

test_examples = pd.read_csv('AItutor_test_data.csv')
results_df = pd.DataFrame(columns=['#temperature', '#top_p', '#presence_penalty', '#freq_penalty',
                                   'Concept', 'Question', 'GPT_Answer', 'Rating', 'Rating_Explanation'])

pd.DataFrame(test_examples.iloc[7]).transpose()

def grid_search_on_examples(test_examples):
  # A summary of the best params and best score for each example
  best_params_for_example = pd.DataFrame(columns=['concept', 'best_params', 'best_score'])

  # Keeping a log of param combos for all questions, the provided answers, and the rating
  results_df = pd.DataFrame(columns=['#temperature', '#top_p', '#presence_penalty', '#freq_penalty',
                                     'Concept', 'Question', 'GPT_Answer', 'Rating', 'Rating_Explanation'])

  for i in range(len(test_examples)):
    example = test_examples.iloc[i]
    f_5 = example[0].split(' ')[:5]
    print(f'Example no. {i} --> {f_5}')
    lesson_params = param_search_for_example(results_df, example.Concept, 'Web Development', 
                         example.Text, example.Code,
                         default_question3)
  
    results_df = lesson_params['results_df']
    results_df.to_csv('logs.csv')
    
  return results_df

result_param_search = grid_search_on_examples(pd.DataFrame(test_examples.iloc[7]).transpose()) # only row 7 due to unexpected rate limitation errors

# l7 = pd.read_csv('/content/logs_all_params_lesson7.csv')
# best_p = l7[l7.Rating == 5][['#temperature', '#top_p', '#presence_penalty', '#freq_penalty']]
# best_p.to_csv('best_params_L7.csv')
